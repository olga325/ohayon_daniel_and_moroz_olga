---
title: "52414 - lab 1"
author: "52414"
date: "4/4/2020"
output: html_document
---

# *Lab 1: Basic Data Wrangling*  
<br/><br/>  
  

**Contents**:  

* Q0) [Submission Instructions](#submission-instructions)  
* Q1) [Data Preparation and Manipulation](#data-preparation-and-manipulation)      
* Q2) [Analysis of Daily New Corona Cases and Deaths](#analysis-of-daily-new-corona-cases-and-deaths)    
* Q3) [Preparing and Analyzing the World Bank Data](#preparing-and-analyzing-the-world-bank-data)
* Q4) [Joining the Datasets](#joining-the-datasets)  
* Q5) [Open Question](#open-question)

<br/><br/>
  
  
### Submission Instructions  
  
This lab will be submitted in pairs using GitHub (if you don't have a pair, please contact us).  
Please follow the steps in the  [GitHub-Classroom Lab 1](https://classroom.github.com/g/oSZNtHq4) to create your group's Lab 1 repository.  
**Important: your team's name must be `FamilyName1_Name1_and_FamilyName2_Name2`**.  
You can collaborate with your partner using the git environment; You can either make commits straight to master, or create individual branches (recommended). However, once done, be sure to merge your branches to master - you will be graded using the most recent master version - your last push and merge before the deadline.   
**Please do not open/review other peoples' repositories - we will be notified by GitHub if you do.**

Your final push should include this Rmd file (with your answers) together with the html file that is outputted automatically by knitr when you knit the Rmd. Anything else will be disregarded. In addition, please adhere to the following file format:    
`Lab_2_FamilyName1_Name1_and_FamilyName2_Name2.Rmd/html`      


<br/><br/>
  
The only allowed libraries are the following (**please do not add your own**):
```{r, include=FALSE}
library('tidyverse')
library(data.table)
```  
<br/><br/>

## A Deeper Dive Into John's Hopkins Corona Database         
    
The John's Hopkins Novel Corona Virus (COVID-19) epidemiological data is compiled by the Johns Hopkins University Center for Systems Science and Engineering (JHU CCSE) from various sources. <br>
The dataset contains data since 22nd of January 2020. For the data and more information about it, please visit [here](https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases).    
  
In this lab you will pick up where we left in lecture 2 and analyze the Corona cases and deaths data.  

### Q1
### Data Preparation and Manipulation   
(25 points)  

1. We first prepare and aggregate the data.   

a. First, load the `Corona Confirmed Cases Narrow`, the `Corona Confirmed Deaths Narrow`, and the `Corona Confirmed Recovered Narrow` datasets directly from the John's Hopkins website.  
The type of the `Date` variable should be date type. (2 pts)      
b. Create new data-frames named `cases.agg`, `deaths.agg`, and `recovered.agg` which aggregate the `sum` of Corona cases, deaths, and recovered respectively over the different countries' provinces. To do this, aggregate `Value` using only the country and date features, ignoring all other features (similarly to what has been shown in `lecture 2`).  
To achieve the aggregation use the `aggregate` function. In addition, order the data-frame first by Country and then by Date (increasing order). The columns of each of the two resulting data-frames should be `Country.Region, Date, Value`. (5pts)   
c. Repeat (b) using `tidyverse` and the pipe. Show that the outputs from the two methods are the same. (5pts)  
d. Using the last day of March as a reference, create a single stacked bar-plot that visualizes the top 10 countries in terms of their Corona cases, and their respected Corona deaths and recovered cases stacked on top of the current sick people in three different colors (each stack should add up to total cases). Make sure that the first bar shows the number of confirmed Corona sick people (`sick = cases - deaths - recovered`). What is the biggest issue with the information presented in this plot? (13pts)

   
  
**Solution:**  
```{r, cache=TRUE}
 # a

corona_cases <- read.csv(url("https://data.humdata.org/hxlproxy/data/download/time_series_covid19_confirmed_global_narrow.csv?dest=data_edit&filter01=merge&merge-url01=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2Fe%2F2PACX-1vTglKQRXpkKSErDiWG6ycqEth32MY0reMuVGhaslImLjfuLU0EUgyyu2e-3vKDArjqGX7dXEBV8FJ4f%2Fpub%3Fgid%3D1326629740%26single%3Dtrue%26output%3Dcsv&merge-keys01=%23country%2Bname&merge-tags01=%23country%2Bcode%2C%23region%2Bmain%2Bcode%2C%23region%2Bsub%2Bcode%2C%23region%2Bintermediate%2Bcode&filter02=merge&merge-url02=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2Fe%2F2PACX-1vTglKQRXpkKSErDiWG6ycqEth32MY0reMuVGhaslImLjfuLU0EUgyyu2e-3vKDArjqGX7dXEBV8FJ4f%2Fpub%3Fgid%3D398158223%26single%3Dtrue%26output%3Dcsv&merge-keys02=%23adm1%2Bname&merge-tags02=%23country%2Bcode%2C%23region%2Bmain%2Bcode%2C%23region%2Bsub%2Bcode%2C%23region%2Bintermediate%2Bcode&merge-replace02=on&merge-overwrite02=on&filter03=explode&explode-header-att03=date&explode-value-att03=value&filter04=rename&rename-oldtag04=%23affected%2Bdate&rename-newtag04=%23date&rename-header04=Date&filter05=rename&rename-oldtag05=%23affected%2Bvalue&rename-newtag05=%23affected%2Binfected%2Bvalue%2Bnum&rename-header05=Value&filter06=clean&clean-date-tags06=%23date&filter07=sort&sort-tags07=%23date&sort-reverse07=on&filter08=sort&sort-tags08=%23country%2Bname%2C%23adm1%2Bname&tagger-match-all=on&tagger-default-tag=%23affected%2Blabel&tagger-01-header=province%2Fstate&tagger-01-tag=%23adm1%2Bname&tagger-02-header=country%2Fregion&tagger-02-tag=%23country%2Bname&tagger-03-header=lat&tagger-03-tag=%23geo%2Blat&tagger-04-header=long&tagger-04-tag=%23geo%2Blon&header-row=1&url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_confirmed_global.csv"), comment.char="#")

corona_deaths <- read.csv(url("https://data.humdata.org/hxlproxy/data/download/time_series_covid19_deaths_global_narrow.csv?dest=data_edit&filter01=merge&merge-url01=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2Fe%2F2PACX-1vTglKQRXpkKSErDiWG6ycqEth32MY0reMuVGhaslImLjfuLU0EUgyyu2e-3vKDArjqGX7dXEBV8FJ4f%2Fpub%3Fgid%3D1326629740%26single%3Dtrue%26output%3Dcsv&merge-keys01=%23country%2Bname&merge-tags01=%23country%2Bcode%2C%23region%2Bmain%2Bcode%2C%23region%2Bsub%2Bcode%2C%23region%2Bintermediate%2Bcode&filter02=merge&merge-url02=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2Fe%2F2PACX-1vTglKQRXpkKSErDiWG6ycqEth32MY0reMuVGhaslImLjfuLU0EUgyyu2e-3vKDArjqGX7dXEBV8FJ4f%2Fpub%3Fgid%3D398158223%26single%3Dtrue%26output%3Dcsv&merge-keys02=%23adm1%2Bname&merge-tags02=%23country%2Bcode%2C%23region%2Bmain%2Bcode%2C%23region%2Bsub%2Bcode%2C%23region%2Bintermediate%2Bcode&merge-replace02=on&merge-overwrite02=on&filter03=explode&explode-header-att03=date&explode-value-att03=value&filter04=rename&rename-oldtag04=%23affected%2Bdate&rename-newtag04=%23date&rename-header04=Date&filter05=rename&rename-oldtag05=%23affected%2Bvalue&rename-newtag05=%23affected%2Binfected%2Bvalue%2Bnum&rename-header05=Value&filter06=clean&clean-date-tags06=%23date&filter07=sort&sort-tags07=%23date&sort-reverse07=on&filter08=sort&sort-tags08=%23country%2Bname%2C%23adm1%2Bname&tagger-match-all=on&tagger-default-tag=%23affected%2Blabel&tagger-01-header=province%2Fstate&tagger-01-tag=%23adm1%2Bname&tagger-02-header=country%2Fregion&tagger-02-tag=%23country%2Bname&tagger-03-header=lat&tagger-03-tag=%23geo%2Blat&tagger-04-header=long&tagger-04-tag=%23geo%2Blon&header-row=1&url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_deaths_global.csv"), comment.char="#")

corona_recovered <- read.csv(url("https://data.humdata.org/hxlproxy/data/download/time_series_covid19_recovered_global_narrow.csv?dest=data_edit&filter01=merge&merge-url01=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2Fe%2F2PACX-1vTglKQRXpkKSErDiWG6ycqEth32MY0reMuVGhaslImLjfuLU0EUgyyu2e-3vKDArjqGX7dXEBV8FJ4f%2Fpub%3Fgid%3D1326629740%26single%3Dtrue%26output%3Dcsv&merge-keys01=%23country%2Bname&merge-tags01=%23country%2Bcode%2C%23region%2Bmain%2Bcode%2C%23region%2Bsub%2Bcode%2C%23region%2Bintermediate%2Bcode&filter02=merge&merge-url02=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2Fe%2F2PACX-1vTglKQRXpkKSErDiWG6ycqEth32MY0reMuVGhaslImLjfuLU0EUgyyu2e-3vKDArjqGX7dXEBV8FJ4f%2Fpub%3Fgid%3D398158223%26single%3Dtrue%26output%3Dcsv&merge-keys02=%23adm1%2Bname&merge-tags02=%23country%2Bcode%2C%23region%2Bmain%2Bcode%2C%23region%2Bsub%2Bcode%2C%23region%2Bintermediate%2Bcode&merge-replace02=on&merge-overwrite02=on&filter03=explode&explode-header-att03=date&explode-value-att03=value&filter04=rename&rename-oldtag04=%23affected%2Bdate&rename-newtag04=%23date&rename-header04=Date&filter05=rename&rename-oldtag05=%23affected%2Bvalue&rename-newtag05=%23affected%2Binfected%2Bvalue%2Bnum&rename-header05=Value&filter06=clean&clean-date-tags06=%23date&filter07=sort&sort-tags07=%23date&sort-reverse07=on&filter08=sort&sort-tags08=%23country%2Bname%2C%23adm1%2Bname&tagger-match-all=on&tagger-default-tag=%23affected%2Blabel&tagger-01-header=province%2Fstate&tagger-01-tag=%23adm1%2Bname&tagger-02-header=country%2Fregion&tagger-02-tag=%23country%2Bname&tagger-03-header=lat&tagger-03-tag=%23geo%2Blat&tagger-04-header=long&tagger-04-tag=%23geo%2Blon&header-row=1&url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_recovered_global.csv"), comment.char="#")

#b
cases.agg <-aggregate(Value ~ Country.Region + Date , corona_cases ,FUN=sum)
cases.agg <- cases.agg[order(cases.agg$Date , decreasing = FALSE),]
deaths.agg <- aggregate(Value ~ Country.Region + Date , corona_deaths ,FUN=sum)
deaths.agg <- deaths.agg[order(deaths.agg$Date , decreasing = FALSE),]
recovered.agg <- aggregate(Value ~ Country.Region + Date , corona_recovered ,FUN=sum)
recovered.agg <- recovered.agg[order(recovered.agg$Date , decreasing = FALSE),]

#c
library(tidyverse)
cases.agg2 <- group_by(corona_cases, Country.Region, Date) %>% summarise(sum = sum(Value))
cases.agg2 <- cases.agg2[order(cases.agg2$Date , decreasing = FALSE),]
deaths.agg2 <- group_by(corona_deaths, Country.Region, Date) %>% summarise(sum = sum(Value))
deaths.agg2 <- deaths.agg2[order(deaths.agg2 $Date , decreasing = FALSE),]
recovered.agg2 <- group_by(corona_recovered, Country.Region, Date) %>% summarise(sum = sum(Value))
recovered.agg2 <- recovered.agg2[order(recovered.agg2 $Date , decreasing = FALSE),]
#d
cases_march <- cases.agg[which(cases.agg$Date == "2020-03-31"),]
deaths_march <- deaths.agg[which(deaths.agg$Date == "2020-03-31"),]
recovered_march <- recovered.agg[which(recovered.agg$Date == "2020-03-31"),]


join1 <- left_join(cases_march,deaths_march,by="Country.Region")
join2 <- left_join(join1, recovered_march)
join2 <- mutate(join2, sick = c(join2$Value.x)-c(join2$Value.y)-c(join2$Value) )
join2 <- head(join2[order(join2$Value.x , decreasing = TRUE),],10)

colors <- c("red", "green", "orange")
countries <- c(as.character(join2$Country.Region))
countries
regions <- c(  join2$Value.y, join2$Value, join2$sick)
#barplot(as.matrix(join2), main = "Top 10 ", names.arg = join2$Country.Region, xlab = "countries", ylab = "total cases", col = colors)

#legend("topleft", regions, cex = 1.3, fill = colors)
```





<br/><br/>  

### Q2
### Analysis of Daily New Corona Cases and Deaths  
20 points

The two datasets (Corona Cases and Deaths) register the value of cases and deaths, respectively, as a cumulative sum for each day. In this question we would like to understand the daily differences between consecutive days.     

a. Add a new column named `Diff` to both the `cases.agg` and the `deaths.agg` data-frames. This new column should register the daily `Value` difference for each country. In other words, the `Diff` column shows how many new cases/deaths each country incurs every day. Hint - diff must be per country. (7pts)  
b. Find the top 10 instances of country and date combinations with the greatest absolute number of new daily Corona cases and deaths (separately). Print the result in a descriptive format. (5pts)  
c. In one figure, plot Italy's new daily Corona cases AND deaths as a function of Date. Choose the plot type you think that makes the most sense. (3pts) 
d. Plot the same graph as in (c), but this time plot the number of new cases on the logarithm scale. What can we learn? (5pts)  

```{r}
 # a
cases.agg <-cases.agg[order(cases.agg$Country.Region, decreasing = FALSE),]
cases.agg <- cases.agg %>%
  group_by(Country.Region) %>%
   mutate(Diff = Value-lag(Value)) 

deaths.agg <- deaths.agg[order(deaths.agg$Country.Region,decreasing = FALSE),]
deaths.agg <- deaths.agg %>%
  group_by(Country.Region) %>%
  mutate(Diff = Value-lag(Value))
#b
cases.agg4 <- head(cases.agg[order(cases.agg$Diff, decreasing = TRUE),],10)
deaths.agg4 <- head(deaths.agg[order(deaths.agg$Diff,decreasing = TRUE),],10)
cases.agg4
deaths.agg4

#c
italy_cases <- cases.agg[which(cases.agg$Country.Region=="Italy"),]
italy_cases

plot(italy_cases$Diff ~ as.Date(italy_cases$Date, "%d%m%y"),type="1"  ,xlab = "dates", ylab = "italy", main =  "Italy", col = "pink" )

#plot(sales$units~as.Date(sales$date,"%d/%m/%y"),type="l",xlab="Date",ylab="Units Sold")

```
**Soluton:**    

YOUR SOLUTION HERE.
Use code blocks and markdown to clearly communicate your work.

<br/><br/>


### Q3
### Preparing and Analyzing the World Bank Data   
25 points

a. Rename the columns of `eco_data`: `country,S_country,feature,feature_code,Y2018V,Y2019V`. (2pts)  
b. Create a new `eco` data-frame whose dimensions are $266 \times 11$. The first column should include the names of the countries in `eco_data.`   
The rest of the columns should be the features with their respective values in `eco_data` for each country from 2018. Print the head of the new data-frame.(8pts)  
c. Select and rename the following columns: `country` as country, `GDP(US currency)` as GDP, `Population ages 65 and above (% of total population)` as pop65, `Population in the largest city (% of urban population)` as pop_city_ratio, `Population, total` as pop_total columns .  (2pts) 
d. Show a table of the five countries with the highest per capita GDP in 2018.     
Next (considering all countries), plot the % of population over 65 vs. log of GDP per capita in 2018, after excluding the 10% countries with the lowest GDP per capita. Using `lm` and `abline`, add a regression line to the plot. What is your conclusion? (13 pts)  
  
```{r}

#a
eco_data <- read.csv(url("https://raw.githubusercontent.com/DataScienceHU/DataAnalysisR_2020/master/data/economic_data.csv"))
names(eco_data)[names(eco_data)== "ן..Country.Name"] <- "country"
names(eco_data)[names(eco_data)== "Country.Code"] <- "S_country"
names(eco_data)[names(eco_data)== "Series.Name"] <- "feature"
names(eco_data)[names(eco_data)== "Series.Code"] <- "feature_code"
names(eco_data)[names(eco_data)== "X2018..YR2018."] <- "Y2018V"
names(eco_data)[names(eco_data)== "X2019..YR2019."] <- "Y2019V"

#b
eco_data<- eco_data[-c(2641:2649),]
eco_wide <-dcast(eco_data, country~feature, value.var="Y2018V")
dim(eco_wide)

#c

names(eco_wide)[names(eco_wide)== "GDP (current US$)"] <- "GDP"
names(eco_wide)[names(eco_wide)== "Population ages 65 and above (% of total population)"] <- "pop65"
names(eco_wide)[names(eco_wide)== "Population in the largest city (% of urban population)"] <- "pop_city_ratio"
names(eco_wide)[names(eco_wide)== "Population, total"] <- "pop_total"

#d
#eco_wide$GDP_per_capital <- ((as.numeric(c(eco_wide$GDP )) / as.numeric(c(eco_wide$pop_total))
eco_wide <-eco_wide[order(eco_wide$`GDP, PPP (current international $)` , decreasing = TRUE),]
top_5 <- head(eco_wide,5)
top_5



x <- round(nrow(eco_wide )%*% 0.1,)
x
eco_wide <- head(eco_wide,-x)

plot(log(as.numeric(eco_wide$`GDP, PPP (current international $)`)),names.arg=as.numeric(eco_wide$pop65), xlab="65+", ylab="GDP",main="plot"  )
```
  
**Solution:** 
  
YOUR SOLUTION HERE.
Use code blocks and markdown to clearly communicate your work.

<br/><br/>  


### Q4
### Joining the Datasets   
20 points

a. Join the `deaths.agg`, `cases.agg`, and `recovered.agg` into one data-frame called `corona`.(5pts)
b. Join the `corona` and `eco` data-frames in a way that will keep the most information regarding the data (but not full join).   
Make sure that no essential data is thrown away (show this). (3pts)
c. Create new columns of normalized `cases`, `deaths`, and `recovered` so they will show the number of cases per 100,000 people for each country.   
Using the last day of March as a reference, create a single stacked bar plot that visualizes the top 10 countries in terms of normalized Corona cases, and their respected normalized Corona deaths and recovered, as done in Q1.   
how is it different from the graph before normalization? (5pts)
d. Using the last day of March as a reference, create a scatter-plot of normalized deaths and cases vs. `pop65`. Limit the plot to show only countries with 15% or more of `pop65`.   
In addition, color the outliers( pop65>24, norm100K_deaths>15) in that plot in red and add to the plot their country names (7pts)
  
  
**Solution:**   

YOUR SOLUTION HERE.
Use code blocks and markdown to clearly communicate your work.

<br/><br/>  



### Q5
### Open Question
10 points
  
Write an interesting research question regarding the Corona outbreak and then follow the steps to answer it using tables and plots. You can use the loaded datasets or any other dataset you find as long as you add the data file to your `lab1` repository so it can be loaded directly from a `url` (e.g. the World Bank). This question will be graded based on creativity, originality, and the novelty of the analysis.   
  
**Solution:**   

YOUR SOLUTION HERE.
Use code blocks and markdown to clearly communicate your work.

<br/><br/>  